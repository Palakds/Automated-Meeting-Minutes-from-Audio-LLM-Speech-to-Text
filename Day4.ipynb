{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "# Models\n",
    "\n",
    "Looking at the lower level API of Transformers - the models that wrap PyTorch code for the transformers themselves.\n",
    "\n",
    "This notebook can run on a low-cost or free T4 runtime.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTDQBZpH25QB"
   },
   "source": [
    "## One more reminder\n",
    "\n",
    "**Pro-tip:**\n",
    "\n",
    "In the middle of running a Colab, you might get an error like this:\n",
    "\n",
    "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
    "\n",
    "This is a super-misleading error message! Please don't try changing versions of packages...\n",
    "\n",
    "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
    "\n",
    "1. Kernel menu >> Disconnect and delete runtime\n",
    "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
    "3. Connect to a new T4 using the button at the top right\n",
    "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
    "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
    "\n",
    "And all should work great - otherwise, ask me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJ1qpySJTtGy",
    "outputId": "f52a8a44-07bc-450c-fd7a-8674fd70c259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. If you haven't already done so, create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions by clicking on the WRITE tab\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yQI6rGLkLrK"
   },
   "source": [
    "### Accessing Llama\n",
    "\n",
    "Yesterday you should have received approval to use this model:\n",
    "\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "You can either use that today, or it's faster if you get approval for this model too.\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "\n",
    "Select this link to see if you need to request approval too. Pick the version of Llama that you want below by commenting out one of these! Or skip Llama altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models and 1 reasoning model\n",
    "\n",
    "# Llama 3.1 is larger and you should already be approved\n",
    "# see here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Llama 3.2 is smaller but you might need to request access again\n",
    "# see here: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
    "\n",
    "#LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "PHI = \"microsoft/Phi-4-mini-instruct\"\n",
    "GEMMA = \"google/gemma-3-270m-it\"\n",
    "QWEN = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "DEEPSEEK = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a room of Data Scientists\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "Visit their model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "At the top of the page are instructions on how to agree to their terms. If possible, you should use the same email as your huggingface account.\n",
    "\n",
    "In my experience approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models.\n",
    "\n",
    "If you have any problems accessing Llama, please see this colab, including some suggestions if you don't get approved by Meta for any reason.\n",
    "\n",
    "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MtWZYZG920F"
   },
   "source": [
    "If the next cell gives you a 403 permissions error, then please check:\n",
    "1. Are you logged in to HuggingFace? Try running `login()` to check your key works\n",
    "2. Did you set up your API key with full read and write permissions?\n",
    "3. If you visit the Llama3.1 page at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B, does it show that you have access to the model near the top?\n",
    "\n",
    "And work through my Llama troubleshooting colab:\n",
    "\n",
    "https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "dc7eba66dfb6437eb2da8ddceda46817",
      "bcbff7507b8b4ee8b6f1f741f0c91f75",
      "d94dcfd8858e4e32890eebc04a36e74c",
      "a8cd287f865a4a39b000e090a8d89e8e",
      "ea5004ffafae4ae2ba3de0eb9bf13917",
      "358e2fbb135d4a29a3a036502af8c4b2",
      "cac499ebbcc444b1bd6fc4e6be5efaaf",
      "b8021d7a50d34bd58b37d213cb8187da",
      "177bf8cccd9048f7970d8c4feb657a28",
      "77622143e86f469da37dadd3261fbbdd",
      "807074f6f7a74068af0a69391ab5e8de",
      "e522f7d85e564270b7c85d0ccb37b82b",
      "650a4a096cf54ea2af8361d2c06dc58f",
      "85442265d0f84616ba0a5364648f1b07",
      "6faa2ede92234965b7a09c244472bccc",
      "732ff12330ea4024b35d3ebd70c14e2b",
      "56d5272fbae7497890e839e809d7696f",
      "b6d8e404a14a4d68af16d843c78b8672",
      "aea0ef4c62af4542befdb284839303c0",
      "6a7fe74120d04c98a9d7a69004764822",
      "8b70da035feb415f99742b7cc92afc20",
      "6b9227ae75754f9fadfa4d3e3c5e32de",
      "bd03679fb0394ec98adb33dd413d0cb6",
      "4d2746a3f4f34d08bade082bafbcb8e1",
      "69d790a4fb77476fbcdfa082ef4976ed",
      "617dba51ac5e48aea92bf4958df949e9",
      "c5b5fe908f064caba0d350137a930b73",
      "9ed24a7688ec47828a01efc8bb0b75f9",
      "193ba49085f449ba85679b340674f808",
      "d03424cc0a1c49e9a5ff05ce99a5863d",
      "183f60b650ac41358dae75b5991c6e57",
      "5f8cf4eb6a9e42759796450d295e2553",
      "fe67c1c9be7347879d2c1b6be22cf9c6"
     ]
    },
    "id": "Zi8YXiwJHF59",
    "outputId": "918d7c1e-fcde-49ef-9508-e469cc23174e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc7eba66dfb6437eb2da8ddceda46817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e522f7d85e564270b7c85d0ccb37b82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd03679fb0394ec98adb33dd413d0cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_HHBEjF8ion",
    "outputId": "2fb4dc22-c182-4ced-f9b9-a075acee1561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
       "            882, 128007,    271,  41551,    264,  22380,    369,    264,   3130,\n",
       "            315,   2956,  57116, 128009]], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "0946691f75ef44be8b837e6656f31387",
      "1ca0efff931848e1806033debfaa4d96",
      "5986b6457204415e9fd138ecb1969d88",
      "e7e5e61719264b02954e4fb9e5ae1a96",
      "7666cfbf7bf644ed8693e91455252e3a",
      "0a94bbfac4d34241a2c3feddfdfc8e1b",
      "a894bc00a466405f846ad8061bb099ef",
      "aed24989eeb24bf093c1e885c2999cc6",
      "eac53733d8a24d289eb39f46c7107472",
      "e2d947290c514037ad72e7144077aefd",
      "ecda8c4e506743e9bda4ae7b1665e600",
      "6a0450135aed429f83fc5b6813c0f1f1",
      "bfb3ffa641d94a5fb3f11b1423f8535a",
      "0e8a34bfab6b41729d5b3f0c3833ff1a",
      "32bac9dd2a8a4d35baee82413fb2778d",
      "d5f4abe664f24949a1aee3bb2a21578f",
      "4118d52b2a03490e829287b67df38664",
      "9944dbd7da514f69b3b761e366a99ebd",
      "c9d1acceb56b428a9f488421bf809b91",
      "01875e273907480286a8e4c84e4fdbb7",
      "7119c47c93d94f979323228eb2d47dc3",
      "227da0bc6c8f48a0894167f0eb26352e",
      "a576d3991bdc4e919118bb11bce015f3",
      "adc3f195df744926bb4b75fc71398aa0",
      "e9ae8e710935432c9df988c38d92e420",
      "14276ad57fb046fcb1fd132abfec2a7f",
      "f83c9cf847de4153a6f1fe9548b64271",
      "3d46df50af6e4b86868597a8dad1fd1b",
      "bd9306042b424636aae83a583772fe92",
      "e0c141603e7244c38ca689ebd240136c",
      "31eb6d3b3a344ad68a6143ba3069ee78",
      "74a3dd33bd6047e988c53711696e64cf",
      "dc22b7800e4c471881df4b859d074a18",
      "098558aa0d8641b5a154840fa4c418f4",
      "8a106d6beb784bd4886b264bbc5a4ad6",
      "d3a00fee98fa45ad98c0dc26dd0cf036",
      "a892cebfca564fb4a97366e3853d1e3b",
      "6fe5b3db3f2a4689bb4edfaa66cadb67",
      "a5e2dc7e31834f1285508fd4949f80df",
      "cc3a0b36424a4ed4af4fbc52d7928ca3",
      "375fab5bcf4441ef93379e0646eee8f4",
      "9b02e1c5d9654784a3798db2b45d1b81",
      "19d99b5920d84f05941588c937592e00",
      "14bf24a576e44b28b70be2664515119e",
      "9ceff88520cf40ef89a423ea178cf17f",
      "1118e87e87714b118fe260089c27a4f7",
      "ea7d8ebbe4ab44e09c15f14b7cae1be1",
      "96a17d33669749dfbb5b0fc6c01e1180",
      "db6bd3154b134728a375ad9ce6079329",
      "d6f9c3c9ca744d2f8f1fa50968c239c4",
      "0c01ee0aae82419eb988adbf8e00d8fa",
      "5e2bbdd95dff46a1842d626046288b92",
      "661b5222e427428ea8e26cd67f5e1849",
      "9d66625c5acf479985eebdf91ab57974",
      "6daef178eaf24e098f6ecb44bc966b5d",
      "6056d9e30a974b33a054f7580bf818a7",
      "633e8371fb8b45fa8ffef55cd0df3dbd",
      "2d7a3c74cb46448ca3ec34d63bff98db",
      "a78e4fceee964616b76409d908ad3b4d",
      "e9918515350d47eca287bc0d92d65f38",
      "d384403c43634b90839b72cb6b1c5af2",
      "c8eb68029ef245a09f39afb89691b9f9",
      "fb25ad38ae47432180e1bb78615827a2",
      "8ef693ae7ee14335a1272689d381f9f6",
      "f40ba65877014849bea234cbcab7c21c",
      "07101557bd204f9881a64c8cf4c30662",
      "c802aa806f6b48fbb43cc07dc092ce13",
      "b441227bcbea4534b073307f9ada7b1b",
      "78b10212a50a4f038e59db3e968e838c",
      "a5cc44800ad04cc6b33da705d6dec262",
      "1f1e84b4703c46678af6d0d0d4a2407e",
      "b3a0f5c5fcaa4728a170d3451c8e5342",
      "fe0dedab34324a15bd4fb4216e866794",
      "da286d6455c64e6ba97f539e39f812f8",
      "ad02683edc7b4e7db9bbf6ddc20e1a3d",
      "fc60eb063c2a4668a54b60e1ad8d405f",
      "31dd307ae9b24f799e04f071e647c5f0",
      "0d0c1f6f4eda455f97e72fd15bbf05c8",
      "42c3767e736e49d8bdcebf01e01ce61f",
      "c74fc1c164fc4cd7b38c0df41359e70e",
      "7cda83f0a1664b58bb3924ef9664c600",
      "641bd5703fdf481a9718c59b20720d5c",
      "ece506d5830642508208caad10d881bd",
      "9a87a70bfd3c45968ca3ba8145789573",
      "7b0fb40e45e24565adc591455e50d3e4",
      "ab518f2e2a4540ae84c742ae55383de4",
      "ad23010bcea8498fa1fcc24f8226d469",
      "8c253e0cf9be4adc86aa1f475f8f5456",
      "89dd64bdad7647d081e5c44d72eeb711",
      "8d04db414eec4583a857f9f9cedf0041",
      "a18a48b6f722454b9483c26946abb189",
      "af135c7ea1b8432ab1feeae1a9f1ef99",
      "5c069a6804404193b7ecf92d70d8c33d",
      "ae1c122cd1a0457ea278cba68a4ece0d",
      "b3f841e7bbc84ad29164d14bd3250437",
      "4baeea44eb9246d29e15f2e66482f5f5",
      "9e6482557b064a9b833095c30e8ee909",
      "5962c9f6296b47cd82272814e3884919",
      "cbe34e62bd534653bde5335b9c22039a"
     ]
    },
    "id": "S5jly421tno3",
    "outputId": "c8cad027-af0c-446c-ce0a-e486837a8fdf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0946691f75ef44be8b837e6656f31387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0450135aed429f83fc5b6813c0f1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a576d3991bdc4e919118bb11bce015f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098558aa0d8641b5a154840fa4c418f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceff88520cf40ef89a423ea178cf17f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6056d9e30a974b33a054f7580bf818a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c802aa806f6b48fbb43cc07dc092ce13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0c1f6f4eda455f97e72fd15bbf05c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89dd64bdad7647d081e5c44d72eeb711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdbYaT8hWXWE",
    "outputId": "883a3278-7f4c-48dd-a0d1-8b323f08aac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5mcojpzrD_y"
   },
   "source": [
    "## Looking under the hood at the Transformer model\n",
    "\n",
    "The next cell prints the HuggingFace `model` object for Llama.\n",
    "\n",
    "This model object is a Neural Network, implemented with the Python framework PyTorch. The Neural Network uses the architecture invented by Google scientists in 2017: the Transformer architecture.\n",
    "\n",
    "While we're not going to go deep into the theory, this is an opportunity to get some intuition for what the Transformer actually is.\n",
    "\n",
    "If you're completely new to Neural Networks, check out my [YouTube intro playlist](https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs) for the foundations.\n",
    "\n",
    "Now take a look at the layers of the Neural Network that get printed in the next cell. Look out for this:\n",
    "\n",
    "- It consists of layers\n",
    "- There's something called \"embedding\" - this takes tokens and turns them into 4,096 dimensional vectors. We'll learn more about this in Week 5.\n",
    "- There are then 16 sets of groups of layers (32 for Llama 3.1) called \"Decoder layers\". Each Decoder layer contains three types of layer: (a) self-attention layers (b) multi-layer perceptron (MLP) layers (c) batch norm layers.\n",
    "- There is an LM Head layer at the end; this produces the output\n",
    "\n",
    "Notice the mention that the model has been quantized to 4 bits.\n",
    "\n",
    "It's not required to go any deeper into the theory at this point, but if you'd like to, I've asked our mutual friend to take this printout and make a tutorial to walk through each layer. This also looks at the dimensions at each point. If you're interested, work through this tutorial after running the next cell:\n",
    "\n",
    "https://chatgpt.com/canvas/shared/680cbea6de688191a20f350a2293c76b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0qmAD5ZtqWA",
    "outputId": "4ac863ca-0f4d-42d2-96a0-62b70e880c7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute this cell and look at what gets printed; investigate the layers\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kx_0SygM_nmA"
   },
   "source": [
    "### And if you want to go even deeper into Transformers\n",
    "\n",
    "In addition to looking at each of the layers in the model, you can actually look at the HuggingFace code that implements Llama using PyTorch.\n",
    "\n",
    "Here is the HuggingFace Transformers repo:  \n",
    "https://github.com/huggingface/transformers\n",
    "\n",
    "And within this, here is the code for Llama 4:  \n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama4/modeling_llama4.py\n",
    "\n",
    "Obviously it's not neceesary at all to get into this detail - the job of an AI engineer is to select, optimize, fine-tune and apply LLMs rather than to code a transformer in PyTorch. OpenAI, Meta and other frontier labs spent millions building and training these models. But it's a fascinating rabbit hole if you're interested!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkYEXzbotcud",
    "outputId": "f7fb99c5-92c8-4e9c-e2c8-05785e1e264b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "            25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "           220,   1627,  10263,    220,   2366,     19,    271, 128009, 128006,\n",
       "           882, 128007,    271,  41551,    264,  22380,    369,    264,   3130,\n",
       "           315,   2956,  57116, 128009, 128006,  78191, 128007,    271,  10445,\n",
       "          1550,    279,  72810,  31649,   1646,    733,    311,  15419,   1980,\n",
       "         18433,    433,    574,  20558,    311,  49229,   1202,  21958,     13,\n",
       "        128009], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OK, with that, now let's run the model!\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "ZXXnANipSUel",
    "outputId": "20b58fb6-5764-4597-8103-7674c6e893ac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell a joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nWhy did the logistic regression model go to therapy?\\n\\nBecause it was struggling to classify its emotions.<|eot_id|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well that doesn't make much sense!\n",
    "# How about this..\n",
    "\n",
    "tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "# Thank you Kuan L. for helping me get this to properly free up memory!\n",
    "# If you select \"Show Resources\" on the top right to see GPU memory, it might not drop down right away\n",
    "# But it does seem that the memory is available for use by new models in the later code.\n",
    "\n",
    "del model, inputs, tokenizer, outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "## A couple of quick notes on the next block of code:\n",
    "\n",
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues. Try experimenting with setting this to False to see what happens. You can read about this argument here:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/chat_templating#what-are-generation-prompts\n",
    "\n",
    "Thank you to student Piotr B for raising the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages, quant=True, max_new_tokens=80):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  if quant:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to(\"cuda\")\n",
    "  else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model).to(\"cuda\")\n",
    "  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596,
     "referenced_widgets": [
      "2ea651287c4f454f9eeb4518456da234",
      "7b80f210e83a4557bc3a805d5c106be8",
      "01a1a95887594a06acb3cce9b10de87d",
      "00f14f9262444b74aaea2bb34c6a1154",
      "bc01734e967f44c9a5f01e5dabae6264",
      "1cfb705699614513acc1c5a8acb3fc80",
      "abb39a0d21d144998821bca0f9749ef1",
      "4998bf6f66f44432b0f50bddbd8ea2cd",
      "9dddc062e2704c978fe64af820f0d48d",
      "25097f6740b54878aa8c26426c988ed3",
      "7311f43b600d4112b8fe7731a597b21b",
      "6804b3a8e18c42058e9057fb6958b470",
      "060152f42bb149099b6252aa200b0212",
      "92fe5b7a8ee74f8c957833e52c7112fa",
      "f88db15e8d3a48a69b1fa17b89974a0d",
      "9f039dd99af74913948ce99066f49656",
      "27fd84716c704743805da65a467c8666",
      "8171db6ec4774670b67dfc38042e5f5a",
      "8ccff50b7166427fbc656a706d83023d",
      "e436958d093847bcb7b1cf04e71a64bb",
      "300ac2b147064b6bbc40a47449445806",
      "a68f142844d04d0c86016bda8b30e419",
      "cfb38631b44d4a35a5ff4f612d15bb03",
      "d143b324bfe94c95847dd0a868b09b37",
      "0a5680c9e57f4727b01317ee4b1ca54f",
      "2f72a09774824b6cad3da1da7bbfcbbe",
      "f4bd997632714010a89e3144fe5b0899",
      "558194a1c94843eca7d0cb45c39da9ef",
      "4b22f7cbd8fd48f38080637fd5bb268b",
      "4d10c778a2474b44bcf121be9d5b8694",
      "cdfd9ad6686547919814bd4475b00ed1",
      "c8707734b9584903934dc9048b0b006b",
      "a35ff062918146bf966b5d7a50d0a2e9",
      "9cb02d1c18d34f1490e167b1b73757c5",
      "f95d496af0b94e57a31f350873036ab9",
      "a3d73a81b4fd4cff81dc7d1a575764a2",
      "c9cc2ea2429c4591b69e7af8c0887813",
      "fff36a47e51e45ab9c1c562879f1752b",
      "e5f138eae92b4dfe841de2b40aa05108",
      "d64163954ec84013ac03ef9a0a6365fb",
      "c12d64454f0d4c8c88ca0cd13070a6c9",
      "eae0b89e9a494d84b7f426ad6ccf205b",
      "ad999b36c5564e128e9be3abb7575159",
      "9db289be2de84801922d42fbc35e5574",
      "254e764a063a4f688a8a336e955cb4b9",
      "d3ea3cc8283c491f90dc71508e8e6b94",
      "56c025fdefbe4a68bab197ed1e88d5c5",
      "e4c0385a550d4c77b6a13492bdefdc57",
      "6a497d1d8c294e3f957579cab3392e9d",
      "74d268189ec1447388c223d3f929cfcf",
      "ccaefa2aed6e455c8eae8a6c2d663b65",
      "70a8bb2ce785447d86d3e381dde39cf7",
      "0d6318e8b5354aa1b78206cebc089af8",
      "54ae448cf3624f1f99dded4d905f258e",
      "312bd40a81484bb6b8c3bc5a574fb41c",
      "09d112d70e984f51ba442fa5f46e4c96",
      "a59d4c48d40a4d4e90a1321087e7ecea",
      "5d588168ab7a4e289e097d2dc94a4c81",
      "f3fc499e98944a3aae5237a74e98b56c",
      "b11720fcac5942d1bffbb6ec8d1f1288",
      "d835a31ed7404ab6ab3e4ee63be7d6a8",
      "61fe5b97e67a40babf62e30ed157ab14",
      "da45110e1c6f413b840d653b0703f4f0",
      "2c2638d1451c4e1abda6c483b137a189",
      "7f02e630787547119a8b4c5e8b6d3b15",
      "4b2c8c90973949df87751f9589582095",
      "8874f3dc3834499ebe9c1cd633781808",
      "ec6cfcb018e3425eabc924260446d536",
      "0257787f13df476d8b71cea0190c5d55",
      "c85247c8587348d8a86904cef76ba04a",
      "0a8cb71cb2d14320bc09d5ac54807aa9",
      "f2d44208a0ab4cf39bf425b6db4ee5d4",
      "7a1bbe9ac2a740be82d6fc57915828dd",
      "f3776c87dfef4d5da1f9f8a93077fe48",
      "4d8517634cd246b39786c8852c4ab8aa",
      "3d771fa6c3144e58932ab728b5f195d3",
      "e44ba21d46154e2594d047ff24fca435",
      "c0075e2a2b2d4fbbae7a10d3e87f0cd9",
      "f372828bdeee4c0ebb768ec2cf7afe2a",
      "dc7ad1ce7b4142a099184fdb575cf58a",
      "ff3afd4147004e7c8cae8dcd943ed874",
      "b95324baf515473283b8c1329d3ad17f",
      "61f7db8dd2f342c98ba5099081c7e702",
      "56a6168f0f574b00af788f3fbadc9627",
      "7fba5361797d42b9bcd8f15837c751d6",
      "1d9e65e6075c4433a1f02b6259c30061",
      "d2a1c6c886234ffaa0f137c9741f1744",
      "cdf25c999c374e70ad9ef456293c01a9",
      "d0f9e79f02a649a68afd098def4c8f3f",
      "28ed3ead018a41879dba1461f8818b73",
      "c8ef1848c9374801833f6ccf69e7cbfd",
      "ec5564aedc714bfab389d96ae49a0c49",
      "e5cb5867ec1849e38ef66268d2ca48e2",
      "b14efefe724742858fd047aa69247945",
      "a0f87211840a450a9978c45b26b587ac",
      "dab9412cf91f44958ce3e2c89b0dce0e",
      "832dfd0ebf9d407db9f24aca576fa1b7",
      "5637cf76daaf43848dfa1f72028b09c9",
      "a5885fd2ec5d4ef5babb10bfbb240c80",
      "2252e13faf084942b466e827327c1317",
      "5480842c519a480599b751cb4fba902f",
      "273b1a4972164b2ca4265b5777016175",
      "3dd551e30def4c9d958dc16ae3f50af5",
      "1af4f1f12d404fd5b9585e720353014d",
      "c10b7bfebf0b4310951e299bc056bd10",
      "41f4329d34354dc38d65c59e165e6178",
      "2c086d20e2e14107b602e5df8e3d72f3",
      "1252a58cf768440eb2add33092fc95be",
      "f5c8e011a9e840b7983c1d03378c84f1",
      "bb8f4688d0854489ad3cff5226aa4527",
      "cfacc574847f4336ab2ad5971b7c83dc",
      "52965c814cac4a288e9578dc042a15ff",
      "015be9a1fefd4535b98900b9d2d18c18",
      "0fcc7cead65740488e13e33544d03132",
      "4d5d2706f53b447982962a533bef0d71",
      "8251ba1939814dc2b99ae4b267e0823c",
      "3e22c173c43c438e8cdc6562802da822",
      "e699d183791545ae83eb706ec4ae69be",
      "fa3a3c5a4acf4be4be7c6c870b9d2825",
      "a30267d1966a4c22a8fbe8d202c865fe",
      "a6d6971b3bbb44b5a517acd4b38aa06f",
      "58a9468fa37b45628d351502f4f985b2",
      "c2d6d39e3d094ef2a7747ba2919424d6",
      "c378334c281547cd9ca8b8cc899e39f0",
      "ab91c72353b34e25a879ab1917a301d7",
      "2dc6df7b24b8417f8a88955e108bda47",
      "e55ec81e9d6747a089b559ff06842400",
      "115711640fca42c3b7b09546cf680f12",
      "4cb8febdb1c04413a3a9d5446efe0460",
      "37a3c0bba35948cbb234eb6f691c339f",
      "fce8236c47814cc8ba4972f48443114c",
      "9e9cfd1eb814465fb9b328f71a5905d5",
      "eecbf31bf8ca4de8814d13e8f887a310",
      "f3c48a5d15ac4e47ad9214fb4a313f16",
      "84c13ca0065e4be7b0999ab0da17a284",
      "4d2881944da8430c87dbf422f31d1c02",
      "bf239914551241b3a878d5864bdf5e19",
      "326129fb8ef747e7948cbaf47c293473",
      "27b8ff73873b4e42ad259af5cb791aae",
      "1929e8d042a2408bae9fe73418342f30",
      "cb8f250f72494542ab80e0b34b2133fb",
      "5a7f2007119848b999fc456b3b79946d",
      "b28da60987de4206b53918d5c7249cb9"
     ]
    },
    "id": "RFjaY4Pdvbfy",
    "outputId": "fd198fe6-1579-466b-ccfd-3ff2c251f588"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea651287c4f454f9eeb4518456da234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6804b3a8e18c42058e9057fb6958b470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb38631b44d4a35a5ff4f612d15bb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb02d1c18d34f1490e167b1b73757c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/15.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254e764a063a4f688a8a336e955cb4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d112d70e984f51ba442fa5f46e4c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/587 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8874f3dc3834499ebe9c1cd633781808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0075e2a2b2d4fbbae7a10d3e87f0cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f9e79f02a649a68afd098def4c8f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2252e13faf084942b466e827327c1317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfacc574847f4336ab2ad5971b7c83dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a9468fa37b45628d351502f4f985b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecbf31bf8ca4de8814d13e8f887a310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>Tell a joke for a room of Data Scientists<|end|><|assistant|>Sure, here's a joke tailored for data scientists:\n",
      "\n",
      "Why did the data scientist break up with the computer?\n",
      "\n",
      "Because it kept dumping data on them, and they just couldn't handle the relationship anymore!<|end|>\n"
     ]
    }
   ],
   "source": [
    "generate(PHI, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxZQmZDCe4Jf"
   },
   "source": [
    "## Accessing Gemma from Google\n",
    "\n",
    "A student let me know (thank you, Alex K!) that Google also now requires you to accept their terms in HuggingFace before you use Gemma.\n",
    "\n",
    "Please visit their model page at this link and confirm you're OK with their terms, so that you're granted access.\n",
    "\n",
    "https://huggingface.co/google/gemma-3-270m-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "28dafc634e9f457fb1c1cb7979e66e5e",
      "c71d92a27a5b438d93952412678469a0",
      "f904572929ac44e492eff2aba23fd019",
      "e724ad0e3cec430d8e976cad85f32774",
      "0941bb36ec9544bdba1e17b0bd239139",
      "dfe9686357484304b0d6012e6034e2a6",
      "bf4fff798b7f49cb924567df2bae205d",
      "bf429e60078a46d688c5489b3be3512c",
      "233b620efddb48698b3629eb8d18ccbe",
      "3258a81c67f142108511e9b53bdaace7",
      "4d8245f599ae44bc833180c82c1a3df8",
      "22bd4d477bcd4014a919177c1c73023f",
      "51eb41cf52c34228a0a1623945b9ffda",
      "48ae536b7eb54ee79900cefe020df384",
      "e9fdc688881b4feba5884bbb8bea096e",
      "5c10c326edd84d49bf2065dcaec3161d",
      "cb5d12854d2a4fa88463a10f6fb349f8",
      "e6ae00bd504f4422bfebcb18eb9842f5",
      "4ec9f7a0016140d08a244af5d767b15b",
      "13920cd268974b3fb7a66a1cb19142a6",
      "52f4792cd7d949819d3c2927f4411387",
      "b0f2b8a8f73348cc8981d82c37d762ff",
      "90207a2fb71b4ede8eb869b129bb18e6",
      "9cc9b9178f8c4c94b161ea6170335bd9",
      "879d0e122e264adb8ef02d67f0b1f04b",
      "76751ef439d649d2b0081dd3c29cf159",
      "5d2d89e7b11846ce9b22e81bc3654819",
      "e5ced066f5374deaa4239a479cd51657",
      "c47336278369481996a584de2c5fe269",
      "81df5553ecd54744aa676a2e95060028",
      "fbecfde2101142fb85f4db8ceb7ca29a",
      "5bd1a35513da468ea539c2ee6bd326ab",
      "302265cd24a54db88194004d3a3364ae",
      "ce88a6f93e3f4efd99cd6fcaf9a610c4",
      "9c3a6b1868c645a3ae0a4e61c5fac17c",
      "60e04a60821c4977a485d07fe0203431",
      "af408acc492a474bbf6193b282b0151e",
      "c1933f3ce3bf418e9a498b609aa54e78",
      "dc56df84bfb6487584681252aa59227b",
      "1c60812699ee44d79cfcd3d0c58d8496",
      "999a530828be4fc1a9a78a454d7ff572",
      "f81de6ac895e47a29c424bc8f935e38c",
      "717b2da166dd4fa6806a39ea49afed51",
      "545ec1a046c14ceda2abd4a5c4a80f36",
      "45e50cdc7f444b178bff33334aee4fbf",
      "3fe68d7c31e74062b76ca10435b94de6",
      "ba9deb0fdcff49778adf26fa3f63a4a9",
      "3a29c0da8ef34aa3b5c830ac612a4462",
      "980df7a95b90417bafd922b742d4a85a",
      "d9e44657e0be427a9c0d4f2411cee8d4",
      "48b0ad120b334f9388418a0dafc0c08a",
      "125e7c0466c440649a6fea046246599f",
      "45d1f85fcf374436a95d1f10da5a8dfa",
      "ca37d63ec65b47ce932942e016708bd2",
      "cff065f8cff64df7af771ffa95d391f0",
      "9aa8695fc7ba40a0ab36158d011d566c",
      "1e46d1a6431e460bb2f19d981aa210cd",
      "9592bf0fe00c40a7996a100c48844a3c",
      "20d7fa308b344ea69470d5cac6d2c95c",
      "9811e99685374198bf6e3397f530a372",
      "99011a856e9944f4a820addc2c324fc1",
      "66efb037650e4841b7a151129ba04a51",
      "d58ca904607b49c1a5707d4c6d3931ba",
      "a8c400a55251422181ae631dc434c45d",
      "3e3707c35ab440289b154b4b9b384d18",
      "28e3b9a681cd41de84d814cf74ffae5f",
      "75357a29cb974c15b860f2bdec258c86",
      "e1e17277c23e434fb9ea080f582e9b8a",
      "2e9e5956995041afa779e6b5c2326b04",
      "f41475edfc16420eaf9a3883673378f7",
      "214a7b096908459490571374ecebf1a2",
      "67ca6d44dcba41fb990315684ca55907",
      "5efd0d874bdd46cd838b9343977d5b86",
      "178f3c3a0f514acdb09215f99610df38",
      "2c44ff275dc64574adc5ae858f9629b5",
      "3af75e40bb8340f4a03330ad6d39d8e8",
      "e70c6a76261f49d9a59674b1bfa486d9",
      "3a495746286c4e189c0fa3dd594d19f5",
      "11d0136b4b7042dd9f4472516ec0e307",
      "aef2a1bc829c4e5fa7fb49027fa15766",
      "6de54a8909bf4e7390bc190e9593311a",
      "b864fc9f0d954b67936d629016e85692",
      "8d72b164e1314c3daa1191069a98bf1f",
      "10946665ab13420198aaa01daf48a1a8",
      "64d3b64b8a794118b105871a1000e438",
      "3ebcaeb48d854f6a804c9ead54fd5e37",
      "f356ccbfeb044ab195ff670840c6660f",
      "99f9c395ebeb4113ae9bf5d81a42ccc0",
      "26b1b1ef5f6e4cb0887c4cacc5061b89",
      "720fca3827cd4a3399ae63eb313e935b",
      "7029811887ee42c5af06a3c7cae92a1e",
      "f4621fae3247429f93ce78aa04ac08aa",
      "5783ffc826904849891e34d3e1fc17b0",
      "dcb81e12772b4023a9196a495d0948fd",
      "a4f4088c38ab4b548d5f307638aa94da",
      "3a75964223284bd99f9c919617f44a13",
      "32b45722c4444c9ea5f12bf160ae122d",
      "178e888ea8374919962388773272a0c6",
      "d7c196be2ca340a9a3ea73e622d8331a"
     ]
    },
    "id": "q1JW41D-viGy",
    "outputId": "7456cec2-5e98-401d-ba94-eeec2fdcdf49"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dafc634e9f457fb1c1cb7979e66e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bd4d477bcd4014a919177c1c73023f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90207a2fb71b4ede8eb869b129bb18e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce88a6f93e3f4efd99cd6fcaf9a610c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e50cdc7f444b178bff33334aee4fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa8695fc7ba40a0ab36158d011d566c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75357a29cb974c15b860f2bdec258c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a495746286c4e189c0fa3dd594d19f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b1b1ef5f6e4cb0887c4cacc5061b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Why don't data scientists ever really get into coding? \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "generate(GEMMA, messages, quant=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "07b95849fc754752af3d9a1e47b97e9d",
      "4428dee373994ef1866b1f2fd9fd4de5",
      "25b4c1e5be894c1d95e932e83b663d4f",
      "bfc981371c7f47978e941d428d09fabd",
      "81eceed082ed4cb9a08f04eaefcb201f",
      "4363b411fe944906b074321a4c938d64",
      "b9318f1d456d4e599cb354b7b3b944c7",
      "bfa6dfc4509047b8b5b1541aefb72cdf",
      "787320301db944308dcf0d75e379b58e",
      "bbdff218326c4ba7894f08ac428a7b7a",
      "803fde6680b14682ab17edddca5f131d"
     ]
    },
    "id": "0m8yjMB3ZTp3",
    "outputId": "07b53197-79c4-4496-8328-994946b680aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b95849fc754752af3d9a1e47b97e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Tell a joke for a room of Data Scientists<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure! Here's a *data scientist-approved* joke with a side of statistical humor:\n",
      "\n",
      "---\n",
      "\n",
      "Why did the data scientist bring a ladder to the conference?\n",
      "\n",
      "Because they heard the *regression* was going to be *on the rise* — and they wanted to *predict* the perfect height for the presentation!\n",
      "\n",
      "*(Bonus points if you catch the pun on \"regression\" and \"ladder\n"
     ]
    }
   ],
   "source": [
    "generate(QWEN, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyTvb4ESS3u_",
    "outputId": "9813c8b6-f9a6-4f4f-b64c-bf543f78f940"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜><｜User｜>Tell a joke for a room of Data Scientists<｜Assistant｜><think>\n",
      "Okay, so I need to come up with a joke for a room full of data scientists. Hmm, that's an interesting challenge. Let me think about how data scientists use humor and why they might find it funny.\n",
      "\n",
      "First, data scientists deal with a lot of numbers and data, so maybe something related to that. Maybe something that's a bit technical but has a funny twist. Or perhaps something that's more about their work environment rather than their actual skills.\n",
      "\n",
      "I remember that humor often involves wordplay or puns. Maybe I can use that. Let me brainstorm some concepts. Data scientists might joke about their tools, the data they work with, or even the people they meet. There's also the aspect of data being messy and requiring creativity to make sense of it.\n",
      "\n",
      "I should consider the audience here. If it's a room of data scientists, they're likely busy, so the joke needs to be concise and quick. It shouldn't be too long or too obscure. It should probably have a punchline that's easy to understand but clever.\n",
      "\n",
      "Let me think about some common data-related phrases or puns. For example, \"What do you call a data scientist?\" could be a good opener, but it's too straightforward. Maybe something that involves their tools or the environment.\n",
      "\n",
      "Wait, there's this one joke about data scientists and their favorite type of food. It goes something like, \"I prefer to eat in public because I'm a data scientist!\" That's a bit of a play on words, using the word \"public\" which is a slang term, and \"data scientist\" which is a play on \"data scientist.\" It's funny because it's relatable and uses a common pun.\n",
      "\n",
      "Let me see if I can come up with another one. Maybe something about their workflow or their approach to problem-solving. Or perhaps a joke about their tools or the tools they use. Let me try that.\n",
      "\n",
      "Another idea: \"Why did the data scientist go to the doctor? To fix the data!!\" That's a bit of a play on \"doctor\" which is a term that can be used negatively, but in this context, it's funny because it's a pun on \"data.\" It's concise and has a clear joke.\n",
      "\n",
      "Or maybe something about their love for numbers. \"What do you call a data scientist? A number cruncher!!\" That's another pun, using \"number cruncher\" as a term and \"data scientist\" as a play\n"
     ]
    }
   ],
   "source": [
    "generate(DEEPSEEK, messages, quant=False, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWCHrOD-1xhp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
